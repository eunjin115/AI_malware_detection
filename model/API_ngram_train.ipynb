{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import time\n",
    "import array\n",
    "import dis\n",
    "import operator\n",
    "import csv\n",
    "import hashlib\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "class NGRAM_features:\n",
    "    def __init__(self, output):\n",
    "        \n",
    "        self.output = output\n",
    "        self.gram = dict()\n",
    "        self.imports = \"\"\n",
    "\n",
    "    def gen_list_n_gram(self, num, asm_list):\n",
    "\n",
    "        for i in range(0, len(asm_list), num):\n",
    "            yield asm_list[i:i+num]\n",
    "\n",
    "    def n_grams(self, num, asm_list, ex_mode):\n",
    "        if ex_mode == 1:\n",
    "            gram = self.gram\n",
    "        elif ex_mode == 0:\n",
    "            gram = dict()\n",
    "\n",
    "        gen_list = self.gen_list_n_gram(num, asm_list)\n",
    "        \n",
    "        for lis in gen_list:\n",
    "            lis = \" \".join(map(str,lis))\n",
    "            try:\n",
    "                gram[lis] += 1 # API 호출 패턴의 개수를 하나 늘림 \n",
    "            except:            # 해당 패턴이 gram에 없는 경우 예외 발생\n",
    "                gram[lis] = 1  # 해당 API 호출 패턴을 새로운 키 값으로 추가 \n",
    "            \n",
    "        return gram\n",
    "\n",
    "\n",
    "    def get_ngram_count(self, headers, grams, label):\n",
    "        patterns = list()\n",
    "        for pat in headers:\n",
    "            try:\n",
    "                patterns.append(grams[pat])\n",
    "            except:\n",
    "                patterns.append(0)\n",
    "        patterns.append(label)\n",
    "        return patterns\n",
    "\n",
    "    def write_csv_header(self, headers):\n",
    "        filepath = self.output\n",
    "        class_ = ['class']\n",
    "        headers = headers + class_\n",
    "\n",
    "        csv_file= open(filepath,\"wa\")\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow(headers)\n",
    "        csv_file.close()\n",
    "\n",
    "    def write_csv_data(self,data):\n",
    "        filepath = self.output\n",
    "        csv_file= open(filepath,\"a\")\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow(data)\n",
    "        csv_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Extracting ngram patterns from files\n",
      "[*] Total length of 3-gram list : 23211\n",
      "[*] Using 2000 grams as features\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "def main():    \n",
    "\n",
    "    num_of_features = 2000\n",
    "\n",
    "    output_file = \"./ngram.csv\"\n",
    "\n",
    "    print '[*] Extracting ngram patterns from files'\n",
    "\n",
    "    ef = NGRAM_features(output_file)\n",
    "    \n",
    "    api_seq=[]\n",
    "    \n",
    "    with open(\"./api_order_mal.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for l in reader:\n",
    "            api_seq.append(l)\n",
    "    for i in range(0,460):\n",
    "        grams = ef.n_grams(3, api_seq[i], 1)\n",
    "    \n",
    "    print \"[*] Total length of 3-gram list :\", len(grams)\n",
    "    sorted_x = sorted(grams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print \"[*] Using %s grams as features\" % (num_of_features)\n",
    "    features = sorted_x[0:num_of_features]\n",
    "    headers = list(chain.from_iterable(zip(*features)))[0:num_of_features]\n",
    "    \n",
    "    \n",
    "    ef.write_csv_header(headers)\n",
    "    \n",
    "    print \"#\" * 80\n",
    "\n",
    "    api_seq=[]\n",
    "    with open(\"./api_order_mal.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for l in reader:\n",
    "            api_seq.append(l)\n",
    "\n",
    "    for i in range(0,461):\n",
    "        grams = ef.n_grams(3, api_seq[i], 0)\n",
    "        gram_count = ef.get_ngram_count(headers, grams, 1)  \n",
    "        all_data = []\n",
    "        all_data.extend(gram_count)   \n",
    "        ef.write_csv_data(all_data)   \n",
    "\n",
    "\n",
    "    api_seq=[]\n",
    "    \n",
    "    with open(\"./api_order_normal.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for l in reader:\n",
    "            api_seq.append(l)\n",
    "\n",
    "    for i in range(0,350):\n",
    "        grams = ef.n_grams(3, api_seq[i], 0)\n",
    "        gram_count = ef.get_ngram_count(headers, grams, 0)  \n",
    "        all_data = []\n",
    "        all_data.extend(gram_count)   \n",
    "        ef.write_csv_data(all_data)   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuckoo",
   "language": "python",
   "name": "mlsec_27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
